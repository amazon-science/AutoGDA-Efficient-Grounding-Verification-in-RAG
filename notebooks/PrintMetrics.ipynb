{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5dab3c-cfc3-4e9e-a587-c286262dcdbc",
   "metadata": {},
   "source": [
    "## Make nice table and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4c5ce-913f-41ed-a7e6-c3855943836a",
   "metadata": {},
   "source": [
    "This notebook just serves to collect results from the different run folders and log files and arrange the values in a tabulare format that can be directly pasted into LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e620a298-0fc2-430e-9f48-c1ddb0dcb3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3b6b64-2cf6-4bcd-aa8c-9e351f9d99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e195e6-bf91-4c42-bbe6-f4d33ed3cf79",
   "metadata": {},
   "source": [
    "## Load Final Eval files (only Auto-GDA lines in the table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046a077-1da0-4074-8965-5da6f8f89a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Obtaining the results for this experiment first required hyperparameter-tuning via ```src\\scripts\\hyper_opt4.py``` and performing the actual evaluation runs via ```run_multiple_seeds.sh```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "073fca83-a6b6-470b-9b92-a61e3a82572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "## Use line according to which teacher model is used.\n",
    "datasets = [(\"ragtruth\", \"-Summary\", \"tasksource\"), (\"ragtruth\", \"-QA\", \"tasksource\"), (\"lfqa\", \"_alignscore\", \"vectara_v2\"), (\"summall\", \"_alignscore\", \"bart-large-tasksource\")] #default (best performing model as teacher, main paper)\n",
    "#datasets = [(\"rtsumm\", \"_gptq\", \"vectara_v2-tasksource\"), (\"rtqa\", \"_gptq\", \"bart-large-tasksource\"), (\"lfqa\", \"_gpt\", \"tasksource\"), (\"summall\", \"_gptq\", \"tasksource-tasksource\")] # learning from gpt (table in Appendix) \n",
    "datasets = [(\"rtsumm\", \"_tasksource_self2\", \"tasksource-tasksource\"), (\"rtqa\", \"_tasksource_self2\", \"tasksource-tasksource\"), (\"lfqa\", \"_tasksource_self2\", \"tasksource-tasksource\"), (\"summall\", \"_tasksource\", \"tasksource-tasksource\")] # self-supervised setup (table in Appendix)\n",
    "models = [\"flan-t5-base\", \"bart-large\", \"tasksource\"]\n",
    "metric = \"roc\"\n",
    "res_mat = np.ones((len(datasets), len(models), 5))*float(\"nan\")\n",
    "for seed in range(1,6):\n",
    "    for idx, (dset, dgroup, pinit) in enumerate(datasets):\n",
    "        log_path = f\"eval_run-{dset}{dgroup}/seed_{seed}-{pinit}/eval_out_unweighted_test_list.json\"\n",
    "        if os.path.exists(log_path):\n",
    "            initial_scores = json.load(open(log_path))\n",
    "            for midx, mkey in enumerate(models):\n",
    "                if dset == \"lfqa\" or dset ==\"summall\":\n",
    "                    if mkey in initial_scores[\"1\"]:\n",
    "                        res_mat[idx, midx, seed-1] = initial_scores[\"1\"][mkey][metric]\n",
    "                else:\n",
    "                    if \"1\" in initial_scores and mkey in initial_scores[\"1\"]:\n",
    "                        res_mat[idx, midx, seed-1] = initial_scores[\"1\"][mkey][metric]\n",
    "                    if \"2\" in initial_scores and mkey in initial_scores[\"2\"]:\n",
    "                        res_mat[idx, midx, seed-1] = initial_scores[\"2\"][mkey][metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e1a8e16-787b-4dcb-985b-0c5538ac01b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.70045348, 0.68086312, 0.6963646 , 0.69294838, 0.69539717],\n",
       "        [0.82492631, 0.80384703, 0.82612803, 0.81558461, 0.80399063],\n",
       "        [0.81735319, 0.83697377, 0.82175195, 0.83367848, 0.84098708]],\n",
       "\n",
       "       [[0.74045868, 0.7189382 , 0.73875175, 0.73876926, 0.73095238],\n",
       "        [0.79333859, 0.79120273, 0.7858806 , 0.76870623, 0.79292279],\n",
       "        [0.80276611, 0.80879289, 0.81872812, 0.81408438, 0.78932511]],\n",
       "\n",
       "       [[0.79450758, 0.82575758, 0.73579545, 0.74621212, 0.78219697],\n",
       "        [0.9157197 , 0.94981061, 0.91098485, 0.94128788, 0.92424242],\n",
       "        [0.92424242, 0.94128788, 0.92518939, 0.90435606, 0.92045455]],\n",
       "\n",
       "       [[0.80777277, 0.81823726, 0.80337692, 0.82633442, 0.82151508],\n",
       "        [0.84676367, 0.85450087, 0.88014331, 0.85082496, 0.86053563],\n",
       "        [0.89174699, 0.88885877, 0.90040316, 0.89098894, 0.87976217]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b64fa83e-3b91-4273-8f9e-3bc1e227f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flan-T5 (Ours) & 0.693 & \\fstd{0.007} & 0.734 & \\fstd{0.008} & 0.777 & \\fstd{0.033} & 0.815 & \\fstd{0.009} & 0.755  \\\\\n",
      "BART (Ours) & 0.815 & \\fstd{0.010} & 0.786 & \\fstd{0.009} & 0.928 & \\fstd{0.015} & 0.859 & \\fstd{0.012} & 0.847  \\\\\n",
      "DeBERTaV2 (Ours) & 0.830 & \\fstd{0.009} & 0.807 & \\fstd{0.010} & 0.923 & \\fstd{0.012} & 0.890 & \\fstd{0.007} & 0.863  \\\\\n"
     ]
    }
   ],
   "source": [
    "means = np.nanmean(res_mat, axis=2)\n",
    "stds = np.nanstd(res_mat, axis=2)\n",
    "print_names = {\"flan-t5-base\": \"Flan-T5 (Ours)\", \"bart-large\": \"BART (Ours)\", \"tasksource\": \"DeBERTaV2 (Ours)\"}\n",
    "for midx, mkey in enumerate(models):\n",
    "    str_print = print_names[mkey]\n",
    "    row_list=[]\n",
    "    for idx, (dset, dgroup, _) in enumerate(datasets):\n",
    "        str_print += r\" & \" + f\"{means[idx, midx]:.3f}\" + r\" & \\fstd{\" + f\"{stds[idx, midx]:.3f}\" +\"}\"\n",
    "        row_list.append(means[idx, midx])\n",
    "    row_list = np.array(row_list)\n",
    "    str_print += f\" & {row_list.mean():.3f}  \"\n",
    "    print(str_print + r\"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a995f1-307e-4dfb-8a30-1a6eb8a95cf0",
   "metadata": {},
   "source": [
    "## Baselines (remaining lines of the table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eba9a99b-fbe0-4341-8a30-67398cea3418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAN-T5 & 0.734 & & 0.708 & & 0.655 & & 0.700 & & 0.699 \\\\\n",
      "DeVERTaV2 & 0.782 & & 0.530 & & 0.645 & & 0.876 & & 0.708 \\\\\n",
      "BART-large & 0.696 & & 0.670 & & 0.821 & & 0.769 & & 0.739 \\\\\n",
      "MiniCheck-T5 & 0.754 & & 0.640 & & 0.741 & & 0.791 & & 0.732 \\\\\n",
      "AlignScore & 0.729 & & 0.822 & & 0.904 & & 0.894 & & 0.837 \\\\\n",
      "Vectara-2.1 & 0.805 & & 0.854 & & 0.648 & & 0.590 & & 0.725 \\\\\n",
      "GPT-4o & 0.892 & & 0.865 & & 0.896 & & 0.880 & & 0.883 \\\\\n",
      "GPT-4o-mini & 0.884 & & 0.833 & & 0.812 & & 0.878 & & 0.852 \\\\\n",
      "GPT-3.5 & 0.706 & & 0.648 & & 0.749 & & 0.814 & & 0.729 \\\\\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dsetlist = [(\"ragtruth\", \"Summary\"), (\"ragtruth\", \"QA\"), (\"lfqa-veri\", \"all\"), (\"summedits\", \"all\")]\n",
    "\n",
    "baselines = {\"flan-t5-base\": \"FLAN-T5\", \"tasksource\": \"DeVERTaV2\",  \"bart-large\": \"BART-large\", \"minicheck-t5\": \"MiniCheck-T5\", \"alignscore\": \"AlignScore\", \"vectara_v2\": \"Vectara-2.1\",\n",
    "                \"gpt-4o\": \"GPT-4o\", \"gpt-4o-mini\": \"GPT-4o-mini\", \"gpt-3.5-turbo\": \"GPT-3.5\"}\n",
    "\n",
    "## Load logfiles\n",
    "res_dict = {}\n",
    "for dset, group in dsetlist:\n",
    "    if dset not in res_dict:\n",
    "        res_dict[dset] = json.load(open(f\"results/eval_baselines_{dset}_nofinetune.json\"))\n",
    "        \n",
    "metric =\"roc\"\n",
    "\n",
    "for baseline, pname in baselines.items():\n",
    "    row_str = pname +\" & \"\n",
    "    row_list = []\n",
    "    for dset, group in dsetlist:\n",
    "        if baseline in res_dict[dset][group]:\n",
    "            row_str += f\"{res_dict[dset][group][baseline][metric]:.3f} & & \"\n",
    "            row_list.append(res_dict[dset][group][baseline][metric])\n",
    "        else:\n",
    "            row_str += \" - & & \"\n",
    "    row_list = np.array(row_list)\n",
    "    row_str += f\"{row_list.mean():.3f} \"\n",
    "    print(row_str+ r\"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b2c22-33fc-440c-a79a-6652d6eb082e",
   "metadata": {},
   "source": [
    "## Ablation study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b47b75-590e-458a-84df-18d446370b83",
   "metadata": {},
   "source": [
    "Collect the results needed for the ablation study table in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f1fdec4-9877-4305-ba3d-d939e948bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "datasets = [(\"ragtruth\", \"-Summary\", \"tasksource\"), (\"ragtruth\", \"-QA\", \"tasksource\"), (\"lfqa\", \"_alignscore\", \"vectara_v2\"), (\"summall\", \"_alignscore\", \"bart-large-tasksource\")]\n",
    "datasets_names = [(\"ragtruth\", \"Summary\"), (\"ragtruth\", \"QA\"),  (\"lfqa-veri\", \"all\"), (\"summedits\", \"all\")] \n",
    "\n",
    "# Unfortunately, we introduced new naming for the runs before performing the random selection experiment.\n",
    "datasets2 = [(\"rtsumm\", \"_tasksource_final2\", \"tasksource-tasksource\"), (\"rtqa\", \"_tasksource_final2\", \"tasksource-tasksource\"), (\"lfqa\", \"_alignscore\", \"vectara_v2-tasksource\"), (\"summall\", \"_alignscore\", \"bart-large-tasksource\")]\n",
    "metric = \"roc\"\n",
    "mkey = \"tasksource\"\n",
    "res_mat_initial_sync = np.ones((len(datasets), 5))*float(\"nan\")\n",
    "res_mat_augmented_sync = np.ones((len(datasets), 5))*float(\"nan\")\n",
    "res_mat_randsel = np.ones((len(datasets), 5))*float(\"nan\")\n",
    "res_mat_nofinetune = np.ones((len(datasets)))*float(\"nan\")\n",
    "res_mat_finetune = np.ones((len(datasets)))*float(\"nan\")\n",
    "## Load logfiles for baselines\n",
    "\n",
    "res_dict_no_ft = {}\n",
    "res_dict_ft = {}\n",
    "for dset, dgroup in datasets_names:\n",
    "    res = json.load(open(f\"results/eval_baselines_{dset}_nofinetune.json\"))\n",
    "    res_dict_no_ft[f\"{dset}-{dgroup}\"] = res[dgroup][mkey][metric]\n",
    "    res = json.load(open(f\"results/eval_baselines_{dset}_finetune.json\"))\n",
    "    res_dict_ft[f\"{dset}-{dgroup}\"] = res[dgroup][mkey][metric]\n",
    "\n",
    "\n",
    "for seed in range(1,6):\n",
    "    for idx, (dset, dgroup, pinit) in enumerate(datasets):\n",
    "        log_path = f\"eval_run-{dset}{dgroup}/seed_{seed}-{pinit}/eval_out_unweighted_test_list.json\"\n",
    "        if os.path.exists(log_path):\n",
    "            initial_scores = json.load(open(log_path))\n",
    "            if \"0\" in initial_scores and  mkey in initial_scores[\"0\"]:\n",
    "                    res_mat_initial_sync[idx, seed-1] = initial_scores[\"0\"][mkey][metric]\n",
    "            if dset == \"lfqa\" or dset ==\"summall\":\n",
    "                if mkey in initial_scores[\"1\"]:\n",
    "                    res_mat_augmented_sync[idx, seed-1] = initial_scores[\"1\"][mkey][metric]\n",
    "            else:\n",
    "                if mkey in initial_scores[\"2\"]:\n",
    "                    res_mat_augmented_sync[idx, seed-1] = initial_scores[\"2\"][mkey][metric]\n",
    "    for idx, (dset, dgroup, pinit) in enumerate(datasets2):\n",
    "        log_path_randsel = f\"eval_run_randomsel-{dset}{dgroup}/seed_{seed}-{pinit}/eval_out_final.json\"\n",
    "        if os.path.exists(log_path_randsel):\n",
    "            log_randsel = json.load(open(log_path_randsel))\n",
    "            #print(log_randsel.keys())\n",
    "            res_mat_randsel[idx, seed-1]  = log_randsel[mkey][\"roc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5dd8afc0-57c1-4b24-bd3b-4100e3846309",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_initial = {}\n",
    "dict_augmented = {}\n",
    "dict_randomsel = {}\n",
    "for idx, (dset, dgroup) in enumerate(datasets_names):\n",
    "    dict_initial[f\"{dset}-{dgroup}\"] = res_mat_initial_sync[idx, ~np.isnan(res_mat_initial_sync[idx])].mean()\n",
    "    dict_augmented[f\"{dset}-{dgroup}\"] = res_mat_augmented_sync[idx, ~np.isnan(res_mat_initial_sync[idx])].mean()\n",
    "    dict_randomsel[f\"{dset}-{dgroup}\"] = res_mat_randsel[idx, ~np.isnan(res_mat_randsel[idx])].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ae59f57-d441-4405-8f28-ec55d2aa1af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Fintuned & 0.782 & 0.530 & 0.645 & 0.876 & 0.708 (0\\%) \\\\\n",
      "Few-Shot Prompt & 0.799 & 0.826 & 0.934 & 0.872 & 0.858 (84\\%) \\\\\n",
      "Augmented-RandomSel & 0.777 & 0.783 & 0.919 & 0.862 & 0.835 (71\\%) \\\\\n",
      "Augmented & 0.837 & 0.867 & 0.925 & 0.883 & 0.878 (96\\%) \\\\\n",
      "Finetuned-Labeled & 0.842 & 0.890 & 0.909 & 0.898 & 0.885 (100\\%) \\\\\n"
     ]
    }
   ],
   "source": [
    "def get_row_str(name, values_dict, std_dict, v_min=0.708, v_max=0.8848):\n",
    "    rowstr = name\n",
    "    values = []\n",
    "    for dset, dgroup in datasets_names:\n",
    "        key = f'{dset}-{dgroup}'\n",
    "        rowstr += f\" & {values_dict[key]:.3f}\"\n",
    "        values.append(values_dict[key])\n",
    "    mean = np.array(values).mean()\n",
    "    rowstr += f\" & {mean:.3f} ({int(100*(mean-v_min)/(v_max-v_min))}\" + r\"\\%)\"\n",
    "    rowstr += r\" \\\\\"\n",
    "    return rowstr\n",
    "    \n",
    "lines = [(\"Non-Fintuned\", res_dict_no_ft), (\"Few-Shot Prompt\", dict_initial), \n",
    "         (\"Augmented-RandomSel\", dict_randomsel), (\"Augmented\", dict_augmented), (\"Finetuned-Labeled\", res_dict_ft)]\n",
    "for name, dicct1 in lines:\n",
    "    print(get_row_str(name, dicct1, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d8b8d-1883-4c39-b8dc-0d683c3f6054",
   "metadata": {},
   "source": [
    "## Ablation study on mutation strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caa8ee-8d76-4083-944e-56ea6224b85a",
   "metadata": {},
   "source": [
    "Collect results obtained using only one mutation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08516374-428c-4b8e-afa4-faf38068c279",
   "metadata": {},
   "source": [
    "TODO: Run more seeds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd9e712f-5eb4-4cbd-9060-2173372d9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "models = [\"tasksource\", \"bart-large\", \"flan-t5-base\"]\n",
    "mutation_list = [\"LLMFillInTheGapsMutation\", \"RephraseMutation\", \"DropSentenceMutation\", \"All\"]\n",
    "dict_list = []\n",
    "n_runs = 5\n",
    "results_matrix = np.ones((5, n_runs, 3))*float(\"nan\")\n",
    "for seed in range(1, 6):\n",
    "    res = json.load(open(f\"eval_run-ragtruth-QA/seed_{seed}-tasksource/eval_out_unweighted_test_list.json\"))\n",
    "    results_matrix[0, seed-1] = np.array([(res[\"0\"][m][\"roc\"] if m in res[\"0\"] else float(\"nan\")) for m in models])  \n",
    "    for idx, mutation in enumerate(mutation_list[:3]):\n",
    "        res = json.load(open(f\"abl_mutation-ragtruth-QA/{mutation}_seed{seed}/eval_out_unweighted_test_list.json\"))\n",
    "        results_matrix[idx+1, seed-1] = np.array([res[\"2\"][m][\"roc\"] for m in models])\n",
    "    res = json.load(open(f\"eval_run-ragtruth-QA/seed_{seed}-tasksource/eval_out_unweighted_test_list.json\"))\n",
    "    results_matrix[4, seed-1] = np.array([res[\"2\"][m][\"roc\"] for m in models])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9404be8d-8102-415c-b1c1-665d87f6e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Only & 0.836  & 0.845  & 0.772  & 0.818 \\\\\n",
      "LLMFillInTheGapsMutation & 0.869  & 0.890  & 0.767  & 0.842 \\\\\n",
      "RephraseMutation & 0.845  & 0.863  & 0.711  & 0.806 \\\\\n",
      "DropSentenceMutation & 0.868  & 0.872  & 0.758  & 0.833 \\\\\n",
      "All & 0.872  & 0.886  & 0.806  & 0.855 \\\\\n"
     ]
    }
   ],
   "source": [
    "mlist_ext =[\"Few-Shot Only\"] + mutation_list\n",
    "for r, vals in enumerate(results_matrix):\n",
    "    rowstr = mlist_ext[r]\n",
    "    #print(rowstr)\n",
    "    for idx, model in enumerate(models + [\"mean\"]):\n",
    "        if model ==\"mean\":\n",
    "            rowstr += f\" & {vals[0, :].mean():.3f} \"\n",
    "        else:\n",
    "            rowstr += f\" & {vals[0, idx].mean():.3f} \"\n",
    "    print(rowstr  +r\"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552469e1-97bf-4d10-aca5-0a055ed55337",
   "metadata": {},
   "source": [
    "## Baseline: Label train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91d990f8-e81d-450c-b9eb-e71e9132ef41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignScore & 0.737 & 0.836 & 0.870 & 0.874 \\\\\n",
      "Vectara-2.1 & 0.814 & 0.879 & 0.879 & 0.805 \\\\\n",
      "GPT-4o & 0.828 & 0.866 & 0.876 & 0.878 \\\\\n"
     ]
    }
   ],
   "source": [
    "model=\"tasksource\"\n",
    "#model=\"bart-large\"\n",
    "metric =\"roc\"\n",
    "dsetlist = [(\"ragtruth\", \"Summary\"), (\"ragtruth\", \"QA\"), (\"lfqa-veri\", \"all\"), (\"summedits\", \"all\")]\n",
    "baseline_labeler = {\"alignscore\": \"AlignScore\", \"vectara_v2\": \"Vectara-2.1\", \"gpt-4o\": \"GPT-4o\"}\n",
    "\n",
    "\n",
    "baseline_vals = {}\n",
    "for baseline, pname in baseline_labeler.items():\n",
    "    baseline_vals[baseline] = []\n",
    "    row_str = pname + \" & \"\n",
    "    for dset, group in dsetlist:\n",
    "        filename = f\"results/eval_baselines_{dset}_finetune_label_{baseline}.json\"\n",
    "        if os.path.exists(filename):\n",
    "            res_dict = json.load(open(filename))\n",
    "            if group in res_dict and model in res_dict[group]:\n",
    "                row_str += f\"{res_dict[group][model][metric]:.3f} & \"\n",
    "                #baseline_vals[baseline].append(res_dict[dset][group][baseline][metric])\n",
    "            else:\n",
    "                row_str += \" - & \"\n",
    "        else:\n",
    "            row_str += \" - & \"\n",
    "    print(row_str[:-2] + r\"\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88dd405-4de7-43e6-be57-1d410291ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DebertaV2 (Ours) 0.837 ± 0.007 0.867 ± 0.007 0.925 ± 0.009 0.890 ± nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fcce9-3a07-4d6f-9dc0-df9b9a144057",
   "metadata": {},
   "source": [
    "## Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a7fb446-8751-4b87-b159-0240c4bd3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = {\"flan-t5-base\": 0.699, \"tasksource\": 0.739, \"bart-large\": 0.739, \"minicheck-t5\": 0.732, \"alignscore\": 0.837, \"vectara_v2\": 0.725, \"gpt-4o\": 0.883} # Add performances hardcoded from main table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c53e0a8-e00d-43b8-85da-abadfe036bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectara & \\wstd{1.57}{0.02} & \\wstd{1.13}{0.03} & \\wstd{1.35}{0.03} & \\wstd{1.03}{0.01} & 1.27 (59\\%) & 72.5\\\\\n",
      "FLAN-T5 & \\wstd{1.71}{0.07} & \\wstd{1.71}{0.07} & \\wstd{1.72}{0.07} & \\wstd{1.71}{0.07} & 1.71 (80\\%) & 69.9\\\\\n",
      "DeBERTaV2 & \\wstd{2.56}{0.03} & \\wstd{1.88}{0.04} & \\wstd{2.15}{0.06} & \\wstd{1.88}{0.09} & 2.12 (100\\%) & 73.9\\\\\n",
      "MiniCheck-T5 & \\wstd{4.50}{0.20} & \\wstd{3.16}{0.06} & \\wstd{3.90}{0.14} & \\wstd{3.22}{0.10} & 3.69 (174\\%) & 73.2\\\\\n",
      "BART-large & \\wstd{4.33}{0.01} & \\wstd{3.62}{0.06} & \\wstd{3.95}{0.09} & \\wstd{3.76}{0.20} & 3.92 (184\\%) & 73.9\\\\\n",
      "AlignScore & \\wstd{5.88}{0.12} & \\wstd{7.55}{0.28} & \\wstd{7.55}{0.35} & \\wstd{1.81}{0.06} & 5.70 (269\\%) & 83.7\\\\\n",
      "GPT-4o & \\wstd{19.80}{0.51} & \\wstd{19.11}{0.44} & \\wstd{21.09}{2.97} & \\wstd{21.89}{1.26} & 20.47 (967\\%) & 88.3\\\\\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dsetlist = [(\"ragtruth\", \"Summary\"), (\"ragtruth\", \"QA\"), (\"lfqa-veri\", \"all\"), (\"summedits\", \"all\")]\n",
    "baseline_labeler = {\"vectara_v2\": \"Vectara\", \"flan-t5-base\": \"FLAN-T5\", \"tasksource\": \"DeBERTaV2\", \"minicheck-t5\": \"MiniCheck-T5\", \"bart-large\": \"BART-large\",\n",
    "                    \"alignscore\": \"AlignScore\", \"gpt-4o\": \"GPT-4o\", }\n",
    "all_results = {}\n",
    "for dset, group in dsetlist:\n",
    "    filename = f\"results/timing_log_50_{dset}_{group}.json\"\n",
    "    if os.path.exists(filename):\n",
    "        all_results[(dset, group)] = json.load(open(filename))\n",
    "\n",
    "for baseline, name in baseline_labeler.items():\n",
    "    str_print = name\n",
    "    mean_list_all = []\n",
    "    for dset, group in all_results.keys():\n",
    "        runtimes = np.array(all_results[(dset, group)][baseline])\n",
    "        rt_mean = runtimes.mean()\n",
    "        rt_std = runtimes.std()/2\n",
    "        str_print += r\" & \\wstd{\" + f\"{rt_mean:.2f}\" + r\"}{\" + f\"{rt_std:.2f}\" +\"}\"\n",
    "        #str_print += \"& \"\n",
    "        mean_list_all.append(rt_mean)\n",
    "    str_print += (f\" & {np.array(mean_list_all).mean():.2f} ({int(np.array(mean_list_all).mean()/0.02117)}\\\\%) & {perfs[baseline]*100:.1f}\" + r\"\\\\\")\n",
    "    print(str_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb17ea-037f-4d62-9f95-299f5f6f3994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
